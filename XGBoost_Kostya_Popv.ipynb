{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost - Kostya Popv.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt, sort\n",
        "from xgboost import XGBClassifier, XGBRegressor, plot_importance\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# split data into X and y. Also very conventional ML. I took the data to be 67% for training and the remaining for test. For asset 1.\n",
        "\n",
        "Asset_1 = Get_N_Asset(1)\n",
        "Asset_1 = Asset_1[:200,:]\n",
        "X, y = Asset_1[:, :-1], Asset_1[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)\n",
        "\n",
        "# fit model on training data\n",
        "model = XGBRegressor(objective ='reg:squarederror',n_estimators = 500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions for test data and somehow I want to evaluate the predictions.\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "print(model.feature_importances_)\n",
        "\n",
        "\n",
        "\"\"\"Here I will iterate over thresholds which contains the importance of each independent variable, which depends on how often\n",
        "   this dependent variable is used  to construct the tree decision models i.e. by how much does a small change in this variable affect performance.\n",
        "   What happens next is that pre-trained model is passed and via transform is reduced to the selected features, which in turn depend on the threshold\n",
        "   (values with higher or equal importance are chosen).\n",
        "   After training the model, I once again select the test data on which I will run my boosting and evaluate the model using the same selected features.\n",
        "   \"\"\"\n",
        "\n",
        "selection = SelectFromModel(model,prefit=True)\n",
        "select_X_train = selection.transform(X_train)\n",
        "# train model\n",
        "selection_model = XGBRegressor(objective ='reg:squarederror')\n",
        "selection_model.fit(select_X_train, y_train)\n",
        "# eval model\n",
        "select_X_test = selection.transform(X_test)\n",
        "predictions = selection_model.predict(select_X_test)\n",
        "# accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Threshold =  , n = {0}\".format(select_X_train.shape[1]))\n",
        "\n",
        "\n",
        "#I plot the importance of parameters, top 5 parameters and the difference between what we predicted and the actual test output data.\n",
        "plot_importance(model)\n",
        "pyplot.show()\n",
        "\n",
        "metrics.mean_absolute_error(y_test, predictions)\n",
        "fig = plt.figure(figsize=(20,10))\n",
        "plt.plot(predictions-y_test)\n",
        "\n",
        "# Plot the top 5 features\n",
        "plot_importance(model, max_num_features=5)\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "score = model.score(X_train,y_train)\n",
        "print(\"Score:{0}\".format(score*100))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vS7F-DlbH0KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OR**"
      ],
      "metadata": {
        "id": "PyQGUQPDH5M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import sort\n",
        "from xgboost import XGBRegressor, plot_importance\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# split data into X and y. Also very conventional ML. I took the data to be 67% for training and the remaining for test. For asset 1.\n",
        "\n",
        "Asset_1 = Get_N_Asset(1)\n",
        "Asset_1 = Asset_1[:200,:]\n",
        "X, y = Asset_1[:, :-1], Asset_1[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)\n",
        "\n",
        "# fit model on training data\n",
        "model = XGBRegressor(objective ='reg:squarederror',n_estimators = 500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions for test data and somehow I want to evaluate the predictions.\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "print(model.feature_importances_)\n",
        "\n",
        "\"\"\"Here I will iterate over thresholds which contains the importance of each independent variable, which depends on how often\n",
        "   this dependent variable is used  to construct the tree decision models i.e. by how much does a small change in this variable affect performance.\n",
        "   What happens next is that pre-trained model is passed and via transform is reduced to the selected features, which in turn depend on the threshold\n",
        "   (values with higher or equal importance are chosen).\n",
        "   After training the model, I once again select the test data on which I will run my boosting and evaluate the model using the same selected features.\n",
        "   \"\"\"\n",
        "\n",
        "thresholds = sort(model.feature_importances_)\n",
        "\n",
        "for thresh in thresholds:\n",
        "  # select features using threshold\n",
        "  selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
        "  select_X_train = selection.transform(X_train)\n",
        "  # train model\n",
        "  selection_model = XGBRegressor(objective ='reg:squarederror',n_estimators = 500)\n",
        "  selection_model.fit(select_X_train, y_train)\n",
        "  # eval model\n",
        "  select_X_test = selection.transform(X_test)\n",
        "  predictions = selection_model.predict(select_X_test)\n",
        "  print(\"Threshold = {0} , n = {1}\".format(thresh, select_X_train.shape[1]))\n",
        "\n",
        "\n",
        "#I plot the importance of parameters, top 5 parameters and the difference between what we predicted and the actual test output data.\n",
        "plot_importance(model)\n",
        "pyplot.show()\n",
        "\n",
        "metrics.mean_absolute_error(y_test, predictions)\n",
        "fig = plt.figure(figsize=(20,10))\n",
        "plt.plot(predictions-y_test)\n",
        "\n",
        "# Plot the top 5 features\n",
        "plot_importance(model, max_num_features=5)\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "score = model.score(X_train,y_train)\n",
        "print(\"Score:{0}\".format(score*100))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M2Zr6TqWH-Qw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}